{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":19018,"databundleVersionId":2703900,"sourceType":"competition"},{"sourceId":1062669,"sourceType":"datasetVersion","datasetId":588377}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install sentencepiece","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-10T12:54:28.726662Z","iopub.execute_input":"2024-09-10T12:54:28.727218Z","iopub.status.idle":"2024-09-10T12:54:29.179439Z","shell.execute_reply.started":"2024-09-10T12:54:28.726968Z","shell.execute_reply":"2024-09-10T12:54:29.178749Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test_labels.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LEN = 128\nBATCH_SIZE = 16 # per TPU core\nTOTAL_STEPS = 2000  # thats approx 4 epochs\nEVALUATE_EVERY = 200\nLR =  1e-5\n\nPRETRAINED_MODEL = 'jplu/tf-xlm-roberta-large'\nD = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.optimizers import Adam\nimport transformers\nfrom transformers import TFAutoModelWithLMHead, AutoTokenizer\nimport logging\n# no extensive logging \nlogging.getLogger().setLevel(logging.NOTSET)\n\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:54:29.180623Z","iopub.execute_input":"2024-09-10T12:54:29.180916Z","iopub.status.idle":"2024-09-10T12:54:34.405111Z","shell.execute_reply.started":"2024-09-10T12:54:29.180890Z","shell.execute_reply":"2024-09-10T12:54:34.404057Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1725972869.632657    1297 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD0910 12:54:29.640925574    1297 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD0910 12:54:29.640939588    1297 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD0910 12:54:29.640942735    1297 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD0910 12:54:29.640945084    1297 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD0910 12:54:29.640947490    1297 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD0910 12:54:29.640949848    1297 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD0910 12:54:29.640952115    1297 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD0910 12:54:29.640954309    1297 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD0910 12:54:29.640956460    1297 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD0910 12:54:29.640958620    1297 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD0910 12:54:29.640960844    1297 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD0910 12:54:29.640963118    1297 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD0910 12:54:29.640965389    1297 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD0910 12:54:29.640967540    1297 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD0910 12:54:29.640969732    1297 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD0910 12:54:29.640971887    1297 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD0910 12:54:29.640974154    1297 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD0910 12:54:29.640976337    1297 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD0910 12:54:29.640978532    1297 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD0910 12:54:29.640980754    1297 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD0910 12:54:29.640982922    1297 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD0910 12:54:29.640985105    1297 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD0910 12:54:29.640987360    1297 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD0910 12:54:29.640989545    1297 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD0910 12:54:29.640991650    1297 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD0910 12:54:29.640993769    1297 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD0910 12:54:29.640995982    1297 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD0910 12:54:29.640998175    1297 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD0910 12:54:29.641000447    1297 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD0910 12:54:29.641003757    1297 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD0910 12:54:29.641006125    1297 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD0910 12:54:29.641008624    1297 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD0910 12:54:29.641011172    1297 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD0910 12:54:29.641013380    1297 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD0910 12:54:29.641015557    1297 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD0910 12:54:29.641017942    1297 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD0910 12:54:29.641020059    1297 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD0910 12:54:29.641022237    1297 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD0910 12:54:29.641024459    1297 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD0910 12:54:29.641026640    1297 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD0910 12:54:29.641028778    1297 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD0910 12:54:29.641030888    1297 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD0910 12:54:29.641033067    1297 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD0910 12:54:29.641035272    1297 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD0910 12:54:29.641037476    1297 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI0910 12:54:29.641206394    1297 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD0910 12:54:29.641218166    1297 ev_posix.cc:113]                      Using polling engine: epoll1\nD0910 12:54:29.651442741    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0910 12:54:29.651452515    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0910 12:54:29.651459923    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0910 12:54:29.651463015    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0910 12:54:29.651466256    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0910 12:54:29.651469443    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD0910 12:54:29.651496893    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0910 12:54:29.651508780    1297 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD0910 12:54:29.651524313    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0910 12:54:29.651549105    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0910 12:54:29.651562307    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0910 12:54:29.651565876    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0910 12:54:29.651570041    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0910 12:54:29.651573138    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0910 12:54:29.651576461    1297 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0910 12:54:29.651579726    1297 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD0910 12:54:29.651608547    1297 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI0910 12:54:29.653162971    1297 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI0910 12:54:29.654457256    1297 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI0910 12:54:29.678229815    1384 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI0910 12:54:29.678273788    1384 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0910 12:54:29.684277259    1297 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-09-10T12:54:29.684262517+00:00\", grpc_status:2}\n","output_type":"stream"},{"name":"stdout","text":"2.16.1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"code","source":"def connect_to_TPU():\n    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n\n    return tpu, strategy, global_batch_size\n\n\ntpu, strategy, global_batch_size = connect_to_TPU()\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:54:34.406903Z","iopub.execute_input":"2024-09-10T12:54:34.407504Z","iopub.status.idle":"2024-09-10T12:54:42.185416Z","shell.execute_reply.started":"2024-09-10T12:54:34.407466Z","shell.execute_reply":"2024-09-10T12:54:42.184450Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Running on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1725972877.639986    1297 service.cc:145] XLA service 0x5b2b15d40610 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1725972877.640052    1297 service.cc:153]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1725972877.640057    1297 service.cc:153]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1725972877.640060    1297 service.cc:153]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1725972877.640066    1297 service.cc:153]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1725972877.640069    1297 service.cc:153]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1725972877.640071    1297 service.cc:153]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1725972877.640074    1297 service.cc:153]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1725972877.640077    1297 service.cc:153]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use the non-experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"D_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n\ndef load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n                      columns=['comment_text', 'toxic']):\n    train_6langs=[]\n    for i in range(len(langs)):\n        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n\n    return train_6langs\n\ndef downsample(df):\n    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n    ds_df= pd.concat([\n        df.query('toxic==1'),\n        df.query('toxic==0').sample(sum(df.toxic))\n    ])\n    \n    return ds_df\n    \n\ntrain_df = pd.concat(load_jigsaw_trans()) ","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:54:42.186649Z","iopub.execute_input":"2024-09-10T12:54:42.186949Z","iopub.status.idle":"2024-09-10T12:54:53.801771Z","shell.execute_reply.started":"2024-09-10T12:54:42.186919Z","shell.execute_reply":"2024-09-10T12:54:53.800621Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.comment_text.values","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:54:53.803763Z","iopub.execute_input":"2024-09-10T12:54:53.804052Z","iopub.status.idle":"2024-09-10T12:54:53.812313Z","shell.execute_reply.started":"2024-09-10T12:54:53.804025Z","shell.execute_reply":"2024-09-10T12:54:53.811410Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array(['KADIN ÇALIŞMAYA ÇALIŞMADAN ÖNCE COCKSUCKER',\n       'Üzgünüm\\n\\nBirinin konuşma sayfasıyla uğraştığım için üzgünüm. Bunu yapmak çok kötüydü. Konuşma sayfalarındaki şablonlara sahip olmanın, onlara hakimiyetinizi kanıtlamanıza nasıl yardımcı olduğunu biliyorum. Yüce yöneticilere boyun eğmem gerektiğini biliyorum. Ama sonra tekrar, dışarıda oynamaya gideceğim .... annenle. 76.122.79.82',\n       'Göt içinde pis annesini sik, kuru!', ...,\n       'Se ele faz uma proposta razoável sobre como alternar o texto, não tenho nenhum problema. No entanto, acredito firmemente que o objetivo oficial da organização deve ser listado na caixa de informações, e não no que alguns outros têm a dizer sobre eles.',\n       '\"\\n\\nDesculpe cara, se meu comportamento é rude. Mas estou pensando de maneira oposta, você está pensando. Na verdade, eu estou em uma imaginação que seção de chumbo deve ser pequena. Anteriormente, você me mostrou um bom exemplo de artigo. Mas, de certa forma, não estou feliz com sua opinião. É por isso que estou fazendo como \"\" Remover informações repetidas e informações sobre datas de lançamento da seção de leads em todos os artigos \"\". Esse é o problema. A partir de agora, eu prometo a você: \"\" Adicionarei todas as informações de datas de lançamento aos meus 12 artigos sobre filmes telugu na Wikipedia em inglês \"\". Na verdade, o número de artigo de meu filme telugu anterior era 24. Mas agora reduzi meu trabalho devido à falta de tempo e cortes de energia por mais de seis horas em Guntur, no estado da Califórnia. Então não se preocupe com isso \"\" Bro \"\", porque eu não quero te machucar novamente. Desculpe!!!. Finalmente entendi claramente. Ei!!! Boas notícias para você. Veja abaixo. [Usuário: Raghusri | Raghusri]] Raghusri \"',\n       '\"\\nNovamente, após numerosos conflitos de edição - talvez algumas pessoas devam olhar para os ataques espúrios que Liz fez contra mim na minha página de discussão antes de alegar que eu cutuquei demais? Novamente, eu realmente não dou a mínima - ela é incapaz de produzir qualquer conteúdo verdadeiro e é puramente uma trabalhadora de redes sociais de POV, assim como você. Eu sempre achei que a Wikipedia prestava um serviço aos leitores, fornecendo artigos bem pesquisados, mas evidentemente esse não é o caso hoje em dia - todos vivemos e aprendemos, talvez um dia eu possa seguir o exemplo de \"\" editores / administradores \"\". como Liz - para ser honesto, acho que não é um ideal que valha a pena cumprir, mas, é claro, sua opinião pode ser diferente. - Bate-papo\\n\\n\"'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])\n    \nPRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\nX_train = regular_encode(train_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n# X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:54:53.813470Z","iopub.execute_input":"2024-09-10T12:54:53.813739Z","iopub.status.idle":"2024-09-10T12:55:24.263495Z","shell.execute_reply.started":"2024-09-10T12:54:53.813714Z","shell.execute_reply":"2024-09-10T12:55:24.262410Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\nDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /jplu/tf-xlm-roberta-large/resolve/main/tokenizer_config.json HTTP/11\" 404 0\nDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /jplu/tf-xlm-roberta-large/resolve/main/config.json HTTP/11\" 200 0\nDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /jplu/tf-xlm-roberta-large/resolve/main/tokenizer_config.json HTTP/11\" 404 0\nDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /jplu/tf-xlm-roberta-large/resolve/main/sentencepiece.bpe.model HTTP/11\" 200 0\n/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 9min 34s, sys: 33.5 s, total: 10min 7s\nWall time: 30.4 s\n","output_type":"stream"}]},{"cell_type":"code","source":"def prepare_mlm_input_and_labels(X):\n    # 15% BERT masking\n    inp_mask = np.random.rand(*X.shape)<0.15 \n    # do not mask special tokens\n    inp_mask[X<=2] = False\n    # set targets to -1 by default, it means ignore\n    labels =  -1 * np.ones(X.shape, dtype=int)\n    # set labels for masked tokens\n    labels[inp_mask] = X[inp_mask]\n    \n    # prepare input\n    X_mlm = np.copy(X)\n    # set input to [MASK] which is the last token for the 90% of tokens\n    # this means leaving 10% unchanged\n    inp_mask_2mask = inp_mask  & (np.random.rand(*X.shape)<0.90)\n    X_mlm[inp_mask_2mask] = 250001  # mask token is the last in the dict\n\n    # set 10% to a random token\n    inp_mask_2random = inp_mask_2mask  & (np.random.rand(*X.shape) < 1/9)\n    X_mlm[inp_mask_2random] = np.random.randint(3, 250001, inp_mask_2random.sum())\n    \n    return X_mlm, labels\n\n\n# use validation and test data for mlm\nX_train_mlm = np.vstack([X_train])\n# masks and labels\nX_train_mlm, y_train_mlm = prepare_mlm_input_and_labels(X_train_mlm)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:55:24.264726Z","iopub.execute_input":"2024-09-10T12:55:24.265032Z","iopub.status.idle":"2024-09-10T12:55:25.735263Z","shell.execute_reply.started":"2024-09-10T12:55:24.265005Z","shell.execute_reply":"2024-09-10T12:55:25.734142Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def create_dist_dataset(X, y=None, training=False):\n    dataset = tf.data.Dataset.from_tensor_slices(X)\n\n    ### Add y if present ###\n    if y is not None:\n        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n        \n    ### Repeat if training ###\n    if training:\n        dataset = dataset.shuffle(len(X)).repeat()\n\n    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n\n    ### make it distributed  ###\n    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n    return dist_dataset\n    \n    \ntrain_dist_dataset = create_dist_dataset(X_train_mlm, y_train_mlm, True)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:55:25.736588Z","iopub.execute_input":"2024-09-10T12:55:25.736874Z","iopub.status.idle":"2024-09-10T12:55:26.614257Z","shell.execute_reply.started":"2024-09-10T12:55:25.736846Z","shell.execute_reply":"2024-09-10T12:55:26.612958Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef create_mlm_model_and_optimizer():\n    with strategy.scope():\n        model = TFAutoModelWithLMHead.from_pretrained(PRETRAINED_MODEL)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n    return model, optimizer\n\n\nmlm_model, optimizer = create_mlm_model_and_optimizer()\nmlm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:55:26.615531Z","iopub.execute_input":"2024-09-10T12:55:26.615837Z","iopub.status.idle":"2024-09-10T12:56:39.144157Z","shell.execute_reply.started":"2024-09-10T12:55:26.615807Z","shell.execute_reply":"2024-09-10T12:56:39.143147Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/models/auto/modeling_tf_auto.py:721: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\nDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /jplu/tf-xlm-roberta-large/resolve/main/config.json HTTP/11\" 200 0\nDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /jplu/tf-xlm-roberta-large/resolve/main/model.safetensors HTTP/11\" 404 0\nDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /jplu/tf-xlm-roberta-large/resolve/main/tf_model.h5 HTTP/11\" 302 0\nDEBUG:filelock:Attempting to acquire lock 135357977753776 on /root/.cache/huggingface/hub/.locks/models--jplu--tf-xlm-roberta-large/bd502c50deb6efcb00fd91768ac2a933716d50b8d85b1fcad21b7d3116ed25c7.lock\nDEBUG:filelock:Lock 135357977753776 acquired on /root/.cache/huggingface/hub/.locks/models--jplu--tf-xlm-roberta-large/bd502c50deb6efcb00fd91768ac2a933716d50b8d85b1fcad21b7d3116ed25c7.lock\nDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs.huggingface.co:443\nDEBUG:urllib3.connectionpool:https://cdn-lfs.huggingface.co:443 \"GET /jplu/tf-xlm-roberta-large/bd502c50deb6efcb00fd91768ac2a933716d50b8d85b1fcad21b7d3116ed25c7?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27tf_model.h5%3B+filename%3D%22tf_model.h5%22%3B&Expires=1726232128&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNjIzMjEyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9qcGx1L3RmLXhsbS1yb2JlcnRhLWxhcmdlL2JkNTAyYzUwZGViNmVmY2IwMGZkOTE3NjhhYzJhOTMzNzE2ZDUwYjhkODViMWZjYWQyMWI3ZDMxMTZlZDI1Yzc~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jJwUNT3jIn-~BD8hFUU4wasTziSohW91OcUIvb9bBa98Bks4gyAychehkW1HRs7mJ8tYKKHasdsrksZz98T6cMpNpEUBlj-lgs4j9zxn1FfYXOZBbhzFQQPUaXCThbUL~2I7V2qjS6czepeDv~tWdfxab6zpcaSw8LbicnvN4F~KpcxuQre60bUSHM0dokzYMUOgNddNHK7E0Ma~22NoMNzAPr50BjQTEczQy~B5g-YhsNPmmmDlhxI1-sLo~Ir8nsmdh7x5mRYHYgQFvxVZkt~ZOG0rnBIZY1vUbTUT0TGIO7pfNn0RvG~LpePhrK7BS9aGvx~gEsTKrCAVICZfjw__&Key-Pair-Id=K3ESJI6DHPFC7 HTTP/11\" 200 3271420488\nDEBUG:filelock:Attempting to release lock 135357977753776 on /root/.cache/huggingface/hub/.locks/models--jplu--tf-xlm-roberta-large/bd502c50deb6efcb00fd91768ac2a933716d50b8d85b1fcad21b7d3116ed25c7.lock\nDEBUG:filelock:Lock 135357977753776 released on /root/.cache/huggingface/hub/.locks/models--jplu--tf-xlm-roberta-large/bd502c50deb6efcb00fd91768ac2a933716d50b8d85b1fcad21b7d3116ed25c7.lock\nI0000 00:00:1725972938.926800    1297 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nAll model checkpoint layers were used when initializing TFXLMRobertaForMaskedLM.\n\nAll the layers of TFXLMRobertaForMaskedLM were initialized from the model checkpoint at jplu/tf-xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"tfxlm_roberta_for_masked_lm\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n roberta (TFXLMRobertaMainL  multiple                  558840832 \n ayer)                                                           \n                                                                 \n lm_head (TFXLMRobertaLMHea  multiple                  257833106 \n d)                                                              \n                                                                 \n=================================================================\nTotal params: 560142482 (2.09 GB)\nTrainable params: 560142482 (2.09 GB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nCPU times: user 48.5 s, sys: 1min 15s, total: 2min 3s\nWall time: 1min 12s\n","output_type":"stream"}]},{"cell_type":"code","source":"def define_mlm_loss_and_metrics():\n    with strategy.scope():\n        mlm_loss_object = masked_sparse_categorical_crossentropy\n\n        def compute_mlm_loss(labels, predictions):\n            per_example_loss = mlm_loss_object(labels, predictions)\n            loss = tf.nn.compute_average_loss(\n                per_example_loss, global_batch_size = global_batch_size)\n            return loss\n\n        train_mlm_loss_metric = tf.keras.metrics.Mean()\n        \n    return compute_mlm_loss, train_mlm_loss_metric\n\n\ndef masked_sparse_categorical_crossentropy(y_true, y_pred):\n    y_true_masked = tf.boolean_mask(y_true, tf.not_equal(y_true, -1))\n    y_pred_masked = tf.boolean_mask(y_pred, tf.not_equal(y_true, -1))\n    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_masked,\n                                                          y_pred_masked,\n                                                          from_logits=True)\n    return loss\n\n            \n            \ndef train_mlm(train_dist_dataset, total_steps=2000, evaluate_every=200):\n    step = 0\n    ### Training lopp ###\n    for tensor in train_dist_dataset:\n        distributed_mlm_train_step(tensor) \n        step+=1\n\n        if (step % evaluate_every == 0):   \n            ### Print train metrics ###  \n            train_metric = train_mlm_loss_metric.result().numpy()\n            print(\"Step %d, train loss: %.2f\" % (step, train_metric))     \n\n            ### Reset  metrics ###\n            train_mlm_loss_metric.reset_states()\n            \n        if step  == total_steps:\n            break\n\n\n@tf.function\ndef distributed_mlm_train_step(data):\n    strategy.run(mlm_train_step, args=(data,))\n\n\n@tf.function\ndef mlm_train_step(inputs):\n    features, labels = inputs\n\n    with tf.GradientTape() as tape:\n        predictions = mlm_model(features, training=True)[0]\n        loss = compute_mlm_loss(labels, predictions)\n\n    gradients = tape.gradient(loss, mlm_model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, mlm_model.trainable_variables))\n\n    train_mlm_loss_metric.update_state(loss)\n    \n\ncompute_mlm_loss, train_mlm_loss_metric = define_mlm_loss_and_metrics()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:56:39.145415Z","iopub.execute_input":"2024-09-10T12:56:39.145706Z","iopub.status.idle":"2024-09-10T12:56:39.233334Z","shell.execute_reply.started":"2024-09-10T12:56:39.145677Z","shell.execute_reply":"2024-09-10T12:56:39.232421Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_mlm(train_dist_dataset, TOTAL_STEPS, EVALUATE_EVERY)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:56:39.235514Z","iopub.execute_input":"2024-09-10T12:56:39.235802Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"DEBUG:absl:`TPUStrategy.run` is called with [args: ((PerReplica:{\n  0: <tf.Tensor 'data:0' shape=(16, 128) dtype=int64>,\n  1: <tf.Tensor 'data_1:0' shape=(16, 128) dtype=int64>,\n  2: <tf.Tensor 'data_2:0' shape=(16, 128) dtype=int64>,\n  3: <tf.Tensor 'data_3:0' shape=(16, 128) dtype=int64>,\n  4: <tf.Tensor 'data_4:0' shape=(16, 128) dtype=int64>,\n  5: <tf.Tensor 'data_5:0' shape=(16, 128) dtype=int64>,\n  6: <tf.Tensor 'data_6:0' shape=(16, 128) dtype=int64>,\n  7: <tf.Tensor 'data_7:0' shape=(16, 128) dtype=int64>\n}, PerReplica:{\n  0: <tf.Tensor 'data_8:0' shape=(16, 128) dtype=int64>,\n  1: <tf.Tensor 'data_9:0' shape=(16, 128) dtype=int64>,\n  2: <tf.Tensor 'data_10:0' shape=(16, 128) dtype=int64>,\n  3: <tf.Tensor 'data_11:0' shape=(16, 128) dtype=int64>,\n  4: <tf.Tensor 'data_12:0' shape=(16, 128) dtype=int64>,\n  5: <tf.Tensor 'data_13:0' shape=(16, 128) dtype=int64>,\n  6: <tf.Tensor 'data_14:0' shape=(16, 128) dtype=int64>,\n  7: <tf.Tensor 'data_15:0' shape=(16, 128) dtype=int64>\n}),)] [kwargs: None]\n/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n  warnings.warn(\n2024-09-10 12:59:33.735074: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.\nI0000 00:00:1725973176.976890    2117 tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(87da689d72d462f2:0:0), session_name()\nI0000 00:00:1725973219.179727    2117 tpu_compile_op_common.cc:507] Found 0 programs. Skip fingerprint registration.\nI0000 00:00:1725973219.200224    2117 tpu_compile_op_common.cc:245] Compilation of 87da689d72d462f2:0:0 with session name  took 42.223280479s and failed\nE0000 00:00:1725973219.200726    2117 tpu_compilation_cache_external.cc:112] XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.00G of 15.48G hbm. Exceeded hbm capacity by 528.67M.\n\nTotal hbm usage >= 16.52G:\n    reserved        530.00M \n    program           9.74G \n    arguments         6.26G \n\nOutput size 6.26G; shares 6.26G with arguments.\n\nProgram hbm requirement 9.74G:\n    global           193.0K\n    scoped            4.00M\n    HLO temp          9.73G (100.0% utilization: Unpadded (7.81G) Padded (7.82G), 19.7% fragmentation (1.92G))\n\n  Largest program allocations in hbm:\n\n  1. Size: 1.91G\n     Operator: op_type=\"SparseSoftmaxCrossEntropyWithLogits\" op_name=\"StatefulPartitionedCall/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[2048,250002]{1,0:T(8,128)}\n     Unpadded size: 1.91G\n     Extra memory due to padding: 880.0K (1.0x expansion)\n     XLA label: fusion.18 = fusion(get-tuple-element.6671, log.13371, fusion.4725, fusion.2, ...(+3)), kind=kLoop, calls=fused_computation.18\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 1.91G\n     Operator: op_type=\"SparseSoftmaxCrossEntropyWithLogits\" op_name=\"StatefulPartitionedCall/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[2048,250002]{1,0:T(8,128)}\n     Unpadded size: 1.91G\n     Extra memory due to padding: 880.0K (1.0x expansion)\n     XLA label: fusion.18 = fusion(get-tuple-element.6671, log.13371, fusion.4725, fusion.2, ...(+3)), kind=kLoop, calls=fused_computation.18\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 1.91G\n     Operator: op_type=\"SparseSoftmaxCrossEntropyWithLogits\" op_name=\"StatefulPartitionedCall/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[2048,250002]{1,0:T(8,128)}\n     Unpadded size: 1.91G\n     Extra memory due to padding: 880.0K (1.0x expansion)\n     XLA label: fusion.18 = fusion(get-tuple-element.6671, log.13371, fusion.4725, fusion.2, ...(+3)), kind=kLoop, calls=fused_computation.18\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._3/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.52 = fusion(get-tuple-element.8486, get-tuple-element.6698, get-tuple-element.8476, get-tuple-element.8477, ...(+3)), kind=kOutput, calls=fused_computation.52\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._16/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.39 = fusion(get-tuple-element.8342, get-tuple-element.6776, get-tuple-element.8332, get-tuple-element.8333, ...(+3)), kind=kOutput, calls=fused_computation.39\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._5/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.50 = fusion(get-tuple-element.8518, get-tuple-element.6710, get-tuple-element.8508, get-tuple-element.8509, ...(+3)), kind=kOutput, calls=fused_computation.50\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._13/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.42 = fusion(get-tuple-element.8294, get-tuple-element.6758, get-tuple-element.8284, get-tuple-element.8285, ...(+3)), kind=kOutput, calls=fused_computation.42\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._20/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.35 = fusion(get-tuple-element.8422, get-tuple-element.6800, get-tuple-element.8412, get-tuple-element.8413, ...(+3)), kind=kOutput, calls=fused_computation.35\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._21/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.34 = fusion(get-tuple-element.8438, get-tuple-element.6806, get-tuple-element.8428, get-tuple-element.8429, ...(+3)), kind=kOutput, calls=fused_computation.34\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._1/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.54 = fusion(get-tuple-element.8229, get-tuple-element.6686, get-tuple-element.8218, get-tuple-element.8219, ...(+3)), kind=kOutput, calls=fused_computation.54\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._10/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.45 = fusion(get-tuple-element.8245, get-tuple-element.6740, get-tuple-element.8235, get-tuple-element.8236, ...(+3)), kind=kOutput, calls=fused_computation.45\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._17/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.38 = fusion(get-tuple-element.8358, get-tuple-element.6782, get-tuple-element.8348, get-tuple-element.8349, ...(+3)), kind=kOutput, calls=fused_computation.38\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._23/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.32 = fusion(get-tuple-element.8470, get-tuple-element.6818, get-tuple-element.8460, get-tuple-element.8461, ...(+3)), kind=kOutput, calls=fused_computation.32\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._2/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.53 = fusion(get-tuple-element.8406, get-tuple-element.6692, get-tuple-element.8396, get-tuple-element.8397, ...(+3)), kind=kOutput, calls=fused_computation.53\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._12/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.43 = fusion(get-tuple-element.8278, get-tuple-element.6752, get-tuple-element.8268, get-tuple-element.8269, ...(+3)), kind=kOutput, calls=fused_computation.43\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._4/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.51 = fusion(get-tuple-element.8502, get-tuple-element.6704, get-tuple-element.8492, get-tuple-element.8493, ...(+3)), kind=kOutput, calls=fused_computation.51\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._7/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.48 = fusion(get-tuple-element.8550, get-tuple-element.6722, get-tuple-element.8540, get-tuple-element.8541, ...(+3)), kind=kOutput, calls=fused_computation.48\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._15/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.40 = fusion(get-tuple-element.8326, get-tuple-element.6770, get-tuple-element.8316, get-tuple-element.8317, ...(+3)), kind=kOutput, calls=fused_computation.40\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._6/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.49 = fusion(get-tuple-element.8534, get-tuple-element.6716, get-tuple-element.8524, get-tuple-element.8525, ...(+3)), kind=kOutput, calls=fused_computation.49\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 32.00M\n     Operator: op_type=\"MatMul\" op_name=\"StatefulPartitionedCall/tfxlm_roberta_for_masked_lm/roberta/encoder/layer_._9/intermediate/dense/Tensordot/MatMul\" source_file=\"dummy_file_name\" source_line=10\n     Shape: f32[16,128,4096]{2,1,0:T(8,128)}\n     Unpadded size: 32.00M\n     XLA label: fusion.46 = fusion(get-tuple-element.8582, get-tuple-element.6734, get-tuple-element.8572, get-tuple-element.8573, ...(+3)), kind=kOutput, calls=fused_computation.46\n     Allocation type: HLO temp\n     ==========================\n\n\n2024-09-10 13:00:19.200767: F tensorflow/core/tpu/kernels/tpu_program_group.cc:90] Check failed: xla_tpu_programs.size() > 0 (0 vs. 0)\nhttps://symbolize.stripped_domain/r/?trace=7b1b83b7ae2c,7b1b83b2c04f,5b2b15f341ef,5b2b15f341ef&map= \n*** SIGABRT received by PID 1297 (TID 2117) on cpu 29 from PID 1297; stack trace: ***\nPC: @     0x7b1b83b7ae2c  (unknown)  (unknown)\n    @     0x7b1a91090387        928  (unknown)\n    @     0x7b1b83b2c050      13648  (unknown)\n    @     0x5b2b15f341f0  (unknown)  (unknown)\nhttps://symbolize.stripped_domain/r/?trace=7b1b83b7ae2c,7b1a91090386,7b1b83b2c04f,5b2b15f341ef&map= \nE0910 13:00:19.239711    2117 coredump_hook.cc:442] RAW: Remote crash data gathering hook invoked.\nE0910 13:00:19.239726    2117 client.cc:269] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.\nE0910 13:00:19.239730    2117 coredump_hook.cc:537] RAW: Sending fingerprint to remote end.\nE0910 13:00:19.239758    2117 coredump_hook.cc:546] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory\nE0910 13:00:19.239768    2117 coredump_hook.cc:598] RAW: Dumping core locally.\n","output_type":"stream"}]},{"cell_type":"code","source":"mlm_model.save_pretrained('./')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}