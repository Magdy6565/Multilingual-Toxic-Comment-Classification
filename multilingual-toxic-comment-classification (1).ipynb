{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":19018,"databundleVersionId":2703900,"sourceType":"competition"},{"sourceId":11650,"sourceType":"datasetVersion","datasetId":8327},{"sourceId":1062669,"sourceType":"datasetVersion","datasetId":588377}],"dockerImageVersionId":30514,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-09T13:04:58.927006Z","iopub.execute_input":"2024-09-09T13:04:58.927365Z","iopub.status.idle":"2024-09-09T13:04:58.939362Z","shell.execute_reply.started":"2024-09-09T13:04:58.927340Z","shell.execute_reply":"2024-09-09T13:04:58.938679Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test_labels.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr.csv\n/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:04:59.032063Z","iopub.execute_input":"2024-09-09T13:04:59.032328Z","iopub.status.idle":"2024-09-09T13:05:03.306398Z","shell.execute_reply.started":"2024-09-09T13:04:59.032305Z","shell.execute_reply":"2024-09-09T13:05:03.305240Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.8/site-packages (4.44.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers) (2024.7.24)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.8/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.6.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:05:03.308559Z","iopub.execute_input":"2024-09-09T13:05:03.308980Z","iopub.status.idle":"2024-09-09T13:05:07.533274Z","shell.execute_reply.started":"2024-09-09T13:05:03.308949Z","shell.execute_reply":"2024-09-09T13:05:07.532211Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/site-packages (0.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LEN = 192 \nDROPOUT = 0.5 # use aggressive dropout\nBATCH_SIZE = 16 # per TPU core\nTOTAL_STEPS_STAGE1 = 2000\nVALIDATE_EVERY_STAGE1 = 200\nTOTAL_STEPS_STAGE2 = 200\nVALIDATE_EVERY_STAGE2 = 10\n\n### Different learning rate for transformer and head ###\nLR_TRANSFORMER = 5e-6\nLR_HEAD = 1e-3\n\nPRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\nPRETRAINED_MODEL = '/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large'\nD = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\nD_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport sentencepiece\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport transformers\nfrom transformers import TFRobertaModel, AutoTokenizer\nimport logging\n# no extensive logging \nlogging.getLogger().setLevel(logging.NOTSET)\n\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:05:07.534798Z","iopub.execute_input":"2024-09-09T13:05:07.535103Z","iopub.status.idle":"2024-09-09T13:05:07.543633Z","shell.execute_reply.started":"2024-09-09T13:05:07.535076Z","shell.execute_reply":"2024-09-09T13:05:07.542903Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:05:07.545444Z","iopub.execute_input":"2024-09-09T13:05:07.545680Z","iopub.status.idle":"2024-09-09T13:05:07.557775Z","shell.execute_reply.started":"2024-09-09T13:05:07.545660Z","shell.execute_reply":"2024-09-09T13:05:07.557138Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def connect_to_TPU():\n    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n\n    return tpu, strategy, global_batch_size\n\n\ntpu, strategy, global_batch_size = connect_to_TPU()\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:05:07.558534Z","iopub.execute_input":"2024-09-09T13:05:07.558756Z","iopub.status.idle":"2024-09-09T13:05:15.286952Z","shell.execute_reply.started":"2024-09-09T13:05:07.558736Z","shell.execute_reply":"2024-09-09T13:05:15.285930Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Running on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"# train =  pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\ntrain = pd.read_csv('/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv')\ntrain['toxic'].sum()\n# train.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:20:42.252821Z","iopub.execute_input":"2024-09-09T13:20:42.253203Z","iopub.status.idle":"2024-09-09T13:20:45.273893Z","shell.execute_reply.started":"2024-09-09T13:20:42.253176Z","shell.execute_reply":"2024-09-09T13:20:45.272928Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"21362"},"metadata":{}}]},{"cell_type":"code","source":"# df.query('toxic==0').sample(sum(df.toxic))\n# Get the number of toxic rows (toxic == 1)\nnum_toxic = train['toxic'].sum()\n\n# Sample an equal number of non-toxic rows (toxic == 0)\nnon_toxic_sample = train[train['toxic'] == 0].sample(num_toxic)\nnon_toxic_sample.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:15:54.876223Z","iopub.execute_input":"2024-09-09T13:15:54.877176Z","iopub.status.idle":"2024-09-09T13:15:54.910817Z","shell.execute_reply.started":"2024-09-09T13:15:54.877134Z","shell.execute_reply":"2024-09-09T13:15:54.909710Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(21384, 8)"},"metadata":{}}]},{"cell_type":"code","source":"def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n                      columns=['comment_text', 'toxic']):\n    train_6langs=[]\n    for i in range(len(langs)):\n        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n\n    return train_6langs\n\ndef downsample(df):\n    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n    ds_df= pd.concat([\n        df.query('toxic==1'),\n        df.query('toxic==0').sample(sum(df.toxic))\n    ])\n    \n    return ds_df\n    \n\ntrain_df = pd.concat(load_jigsaw_trans()) \nval_df = pd.read_csv(D+'validation.csv')\ntest_df = pd.read_csv(D+'test.csv')\nsub_df = pd.read_csv(D+'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:05:15.288190Z","iopub.execute_input":"2024-09-09T13:05:15.288439Z","iopub.status.idle":"2024-09-09T13:05:35.566345Z","shell.execute_reply.started":"2024-09-09T13:05:15.288417Z","shell.execute_reply":"2024-09-09T13:05:35.565150Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:12:00.106943Z","iopub.execute_input":"2024-09-09T13:12:00.107325Z","iopub.status.idle":"2024-09-09T13:12:00.113232Z","shell.execute_reply.started":"2024-09-09T13:12:00.107298Z","shell.execute_reply":"2024-09-09T13:12:00.112350Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(256496, 2)"},"metadata":{}}]},{"cell_type":"code","source":"train_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:34:57.154087Z","iopub.execute_input":"2024-09-09T13:34:57.154518Z","iopub.status.idle":"2024-09-09T13:34:57.170285Z","shell.execute_reply.started":"2024-09-09T13:34:57.154488Z","shell.execute_reply":"2024-09-09T13:34:57.169402Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                             comment_text  toxic\n40468   24/7\\n\\nArrêtez d'apporter des modifications i...      1\n46720   \"'mejorar la suerte de los pobres, pero afianz...      0\n46202   Heil Hitler! Heil Hitler! Heil Hitler! Heil Hi...      1\n56165   \"::: Cher Samboy, je ne crains pas du tout les...      0\n144634  J'ai ajouté l'invasion de l'Irak en 2003, car ...      0\n57729   Me gustaría trasladar a los protestantes del U...      0\n88100   Şimdi kadeh kaldırıyoruz\\n\\nha ha öylesine tos...      1\n13301   Красная ссылка\\n\\nВы недавно заново создали сс...      0\n67258   ¡Maldito infierno!\\nEres tan idiota. ¡Deja de ...      1\n29805   Muy buen contribuyente? Mierda. La camarilla n...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>40468</th>\n      <td>24/7\\n\\nArrêtez d'apporter des modifications i...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>46720</th>\n      <td>\"'mejorar la suerte de los pobres, pero afianz...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46202</th>\n      <td>Heil Hitler! Heil Hitler! Heil Hitler! Heil Hi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>56165</th>\n      <td>\"::: Cher Samboy, je ne crains pas du tout les...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>144634</th>\n      <td>J'ai ajouté l'invasion de l'Irak en 2003, car ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57729</th>\n      <td>Me gustaría trasladar a los protestantes del U...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>88100</th>\n      <td>Şimdi kadeh kaldırıyoruz\\n\\nha ha öylesine tos...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13301</th>\n      <td>Красная ссылка\\n\\nВы недавно заново создали сс...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>67258</th>\n      <td>¡Maldito infierno!\\nEres tan idiota. ¡Deja de ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29805</th>\n      <td>Muy buen contribuyente? Mierda. La camarilla n...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])\n    \n\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\nX_train = regular_encode(train_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\nX_val = regular_encode(val_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(test_df.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n\ny_train = train_df.toxic.values.reshape(-1,1)\ny_val = val_df.toxic.values.reshape(-1,1)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:05:35.567551Z","iopub.execute_input":"2024-09-09T13:05:35.567852Z","iopub.status.idle":"2024-09-09T13:06:17.181078Z","shell.execute_reply.started":"2024-09-09T13:05:35.567829Z","shell.execute_reply":"2024-09-09T13:06:17.179927Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 8min 36s, sys: 23.9 s, total: 9min\nWall time: 41.6 s\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_dist_dataset(X, y=None, training=False):\n    dataset = tf.data.Dataset.from_tensor_slices(X)\n\n    ### Add y if present ###\n    if y is not None:\n        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n        \n    ### Repeat if training ###\n    if training:\n        dataset = dataset.shuffle(len(X)).repeat()\n\n    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n\n    ### make it distributed  ###\n    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n    return dist_dataset\n    \n\ntrain_dist_dataset = create_dist_dataset(X_train, y_train, True)\nval_dist_dataset   = create_dist_dataset(X_val)\ntest_dist_dataset  = create_dist_dataset(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:06:17.182435Z","iopub.execute_input":"2024-09-09T13:06:17.182713Z","iopub.status.idle":"2024-09-09T13:06:18.130949Z","shell.execute_reply.started":"2024-09-09T13:06:17.182689Z","shell.execute_reply":"2024-09-09T13:06:18.129796Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:10:29.740936Z","iopub.execute_input":"2024-09-09T13:10:29.741297Z","iopub.status.idle":"2024-09-09T13:10:29.747283Z","shell.execute_reply.started":"2024-09-09T13:10:29.741266Z","shell.execute_reply":"2024-09-09T13:10:29.746306Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(256496, 192)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\ndef create_model_and_optimizer():\n    with strategy.scope():\n        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)  \n#         transformer_layer = TransformerLayer(base_transformer)\n        model = build_model(transformer_layer)\n        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n        optimizer_head = Adam(learning_rate=LR_HEAD)\n    return model, optimizer_transformer, optimizer_head\n\n\ndef build_model(transformer):\n    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n    # Huggingface transformers have multiple outputs, embeddings are the first one\n    # let's slice out the first position, the paper says its not worse than pooling\n    \n    x = transformer(inp)[0][:, 0, :]  \n    x = Dropout(DROPOUT)(x)\n    ### note, adding the name to later identify these weights for different LR\n    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n    model = Model(inputs=[inp], outputs=[out])\n    \n    return model\n\n\nmodel, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation['lang'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['toxic'].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train['toxic'] == 0).sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Example DataFrame\n# test = pd.read_csv('/path/to/your/test.csv')\n\n# Calculate total number of entries\ntotal_entries = len(validation)\n\n# Iterate over each unique language and calculate its percentage\nfor lang in validation['lang'].unique():\n    lang_count = np.sum(validation['lang'] == lang)\n    lang_percentage = (lang_count / total_entries) * 100\n    print(f\"{lang} : {lang_percentage:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import MarianMTModel, MarianTokenizer\nimport os\n\n# Function to translate text\ndef translate_text(text, model_name):\n    try:\n        tokenizer = MarianTokenizer.from_pretrained(model_name)\n        model = MarianMTModel.from_pretrained(model_name)\n\n        # Tokenize and translate the text\n        translated = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True, max_length=2400)\n        translated_tokens = model.generate(**translated)\n        translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n        return translated_text\n    except Exception as e:\n        print(f\"Error translating text: {text}. Exception: {e}\")\n        return None  # Return None if an error occurs\n\n# Load dataset\n# df = pd.read_csv('/content/sample_data/jigsaw-toxic-comment-train.csv')\ndf = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n\n# Extract comments and labels\ncomments_to_translate = df['comment_text'].tolist()\nlabels = df['toxic'].tolist()\n\n# Define MarianMT models for the target languages\nmodels = {\n    \"es\": \"Helsinki-NLP/opus-mt-en-es\",   # English to Spanish\n    \"tr\": \"Helsinki-NLP/opus-mt-en-trk\",  # English to Turkish (corrected)\n    \"ru\": \"Helsinki-NLP/opus-mt-en-ru\",   # English to Russian\n    \"it\": \"Helsinki-NLP/opus-mt-en-it\",   # English to Italian\n    \"fr\": \"Helsinki-NLP/opus-mt-en-fr\"    # English to French\n}\n\n# Set batch size\nbatch_size = 100  # Modify this based on available resources\nstart_idx = 0\n\n# Check if the output file already exists and resume if needed\noutput_file = 'translated_dataset.csv'\nif os.path.exists(output_file):\n    translated_df = pd.read_csv(output_file)\n    start_idx = len(translated_df) // len(models)  # Calculate start index based on languages translated\nelse:\n    translated_df = pd.DataFrame(columns=['comment', 'toxic', 'language'])\n\n# Process the dataset in batches\nfor start in range(start_idx, len(comments_to_translate), batch_size):\n    end = min(start + batch_size, len(comments_to_translate))\n    translated_data = []\n\n    for i, text in enumerate(comments_to_translate[start:end]):\n        label = labels[start + i]\n\n        # Translate to each language\n        for lang, model_name in models.items():\n            print(f\"Translating comment {start + i + 1} to {lang}...\")\n            translation = translate_text(text, model_name)\n            if translation:\n                translated_data.append({'comment': translation, 'toxic': label, 'language': lang})\n\n    # Convert batch to DataFrame and append to the existing DataFrame\n    batch_df = pd.DataFrame(translated_data)\n    translated_df = pd.concat([translated_df, batch_df], ignore_index=True)\n\n    # Save the translated dataset after each batch\n    translated_df.to_csv(output_file, index=False)\n    print(f\"Batch {start // batch_size + 1} saved to {output_file}\")\n\nprint(\"All translations completed and saved.\")\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train Shape is  {train.shape[0]}\")\nprint(f\"Validation Shape is  {validation.shape[0]}\")\nprint(f\"Test Shape is  {test.shape[0]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check max len of comment_text column to use this for padding in future\npad_len = train['comment_text'].apply(lambda x:len(str(x).split())).max()\nprint('max len of comment_text column',pad_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preperation","metadata":{}},{"cell_type":"code","source":"texts = train.comment_text.values\nlabels = train.toxic.values\n\n# Step 1: Split into training and temporary sets\nx_train, x_temp, y_train, y_temp = train_test_split(\n    texts, labels, stratify=labels, test_size=0.3, random_state=42, shuffle=True\n)\n\n# Step 2: Split the temporary set into validation and test sets\nx_valid, x_test, y_valid, y_test = train_test_split(\n    x_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=42, shuffle=True\n)\n\n# Check the sizes of each set\nprint(f\"Training set size: {len(x_train)}\")\nprint(f\"Validation set size: {len(x_valid)}\")\nprint(f\"Test set size: {len(x_test)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(xtrain),len(xvalid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenisation and Padding with max len of words in curpus","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using keras tokenizer  --> word-level tokenizer\n\n\n\n\n# The Tokenizer from Keras is a tool used to preprocess text data \n# for machine learning models. It converts text into sequences of integers\n# where each integer represents a unique word in the text corpus.\n# Here's a step-by-step explanation of how it works in your code\n\ntoken = text.Tokenizer(num_words = None)\nmax_len = 2400 # bec biggest comment in training is 2321\n# xtest = test.content.values\n# fit_on_texts() builds the word index from the provided texts.\n# It scans through all the text samples (xtrain, xvalid, and xtest),\n# creating a vocabulary dictionary where each word is mapped to a unique integer.\n# This mapping is used later to convert the text into sequences of integers.\n\n\ntoken.fit_on_texts(list(x_train) + list(x_valid) + list(x_test))\n\nx_train_seq = token.texts_to_sequences(x_train)\nx_valid_seq = token.texts_to_sequences(x_valid)\nx_test_seq = token.texts_to_sequences(x_test)\n# texts_to_sequences() converts each text sample into a sequence of integers \n# based on the word index created during fit_on_texts(). Each word in the text is\n# replaced by its corresponding integer from the word index. \n# If a word is not found in the word index (e.g., it wasn't in the training data), it is ignored.\n\n\n#zero pad the sequences\nx_train_pad = pad_sequences(x_train_seq,maxlen = max_len)\nx_valid_pad = pad_sequences(x_valid_seq,maxlen = max_len)\nx_test_pad = pad_sequences(x_test_seq,maxlen = max_len)\n# pad_sequences() ensures that all sequences have the same length (max_len). \n# Sequences shorter than max_len are padded with zeros at the beginning,\n# while sequences longer than max_len are truncated. This step is crucial \n# for ensuring that the input data has a consistent shape when fed into a neural network.\nword_index = token.word_index\n# word_index is a dictionary where keys are words and values are their corresponding integer indices. \n# This index helps in understanding the mapping between words and integers used in the sequences.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1: classification on basic RNN Network","metadata":{}},{"cell_type":"code","source":"len(word_index) + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    \n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                     300,\n                     input_length=max_len))\n    model.add(SimpleRNN(100))\n    model.add(Dense(1,activation = 'sigmoid'))\n    model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, Dense, Input\nfrom tensorflow.keras.models import Model\n\n# Define the RNN cell manually\nclass CustomRNNCell(tf.keras.layers.Layer):\n    def __init__(self, units, **kwargs):\n        super(CustomRNNCell, self).__init__(**kwargs)\n#         ensures that the parent class (Layer) is properly initialized,\n#         allowing the cell to function as a layer within a model.\n        self.units = units\n\n    def build(self, input_shape):\n#         The build method is called automatically when the layer is first used \n        self.W = self.add_weight(shape=(input_shape[-1], self.units), initializer='glorot_uniform', name='W')\n        \n        self.U = self.add_weight(shape=(self.units, self.units), initializer='orthogonal', name='U')\n        self.b = self.add_weight(shape=(self.units,), initializer='zeros', name='b')\n        # Print the shapes of the weights\n        print(f\"W shape: {self.W.shape}\")\n        print(f\"U shape: {self.U.shape}\")\n        print(f\"b shape: {self.b.shape}\")\n\n\n    def call(self, inputs, states):\n#         The call method defines the forward pass of the RNN cell. It takes two inputs:\n\n        prev_state = states[0]\n        print(f\" prev state dims {prev_state.shape}\")\n        h = tf.tanh(tf.matmul(inputs, self.W) + tf.matmul(prev_state, self.U) + self.b)\n        return h, [h]\n\n# Build the custom RNN layer\nclass CustomRNN(tf.keras.layers.Layer):\n    def __init__(self, units, **kwargs):\n        super(CustomRNN, self).__init__(**kwargs)\n        self.units = units\n        self.cell = CustomRNNCell(units)\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        time_steps = tf.shape(inputs)[1]\n\n        # Initialize state and TensorArray for outputs\n        state = [tf.zeros((batch_size, self.units))]\n        outputs = tf.TensorArray(tf.float32, size=time_steps)\n\n        # Loop through time steps\n        for t in tf.range(time_steps):\n            x = inputs[:, t, :]\n            output, state = self.cell(x, state)\n            outputs = outputs.write(t, output)\n\n        # Stack outputs and transpose to match the expected shape\n        outputs = outputs.stack()\n        outputs = tf.transpose(outputs, [1, 0, 2])  # (batch_size, time_steps, units)\n        return outputs\n\n# Define the model\nvocab_size = len(word_index) + 1\nembedding_dim = 300\nmax_len = 2400  # Example length\nrnn_units = 100\n\n# x --> emb  --> 2400*300  X  W (300 * 100) -->  2400*100 + ( (prev_state*U) -->   2400*100      * 100*100 \n#\n\ninputs = Input(shape=(max_len,))\n# pdb.set_trace()\nwith strategy.scope():\n    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)(inputs)\n    print(x.shape)\n    x = CustomRNN(rnn_units)(x)\n    x = tf.reduce_mean(x, axis=1)  # Aggregating the outputs (e.g., via mean or sum)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using strategy to run the TPU\nmodel.fit(x_train_pad,y_train,epochs = 5,batch_size = 64*strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(x_test_pad)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming predictions are probabilities, you might want to convert them to binary class labels\n# For binary classification, you can use a threshold of 0.5 to get class labels\npredicted_labels = (predictions > 0.5).astype(int)\n\n# Create a DataFrame\ndf_predictions = pd.DataFrame(predicted_labels, columns=['Predicted_Label'])\n\n# Save to CSV\ndf_predictions.to_csv('predictions.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_val = model.predict(x_test_pad)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score = roc_auc_score(y_test,pred_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# 2 : classification with pretrained glove word Embedding with Basic LSTM Model","metadata":{}},{"cell_type":"code","source":"# load glove vector in a dictionary\n\nembeddings_index = {}\nf = open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(value) for value in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found {} word vectors'.format(len(embeddings_index)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create an embedding metrics for the words which are part of our datasets\nembedding_metrics = np.zeros((len(word_index) + 1,300))\nfor word,i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_metrics[i] = embedding_vector\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_metrics.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    \n    #simple LSTM Model\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n             300, weights = [embedding_metrics],input_length = max_len,trainable = False))\n    model.add(LSTM(100,dropout = 0.3, recurrent_dropout = 0.3))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train_pad,ytrain,epochs = 5, batch_size = 64*strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_pred = model.predict(x_valid_pad)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy = roc_auc_score(yvalid,lstm_pred)\n# model_accuracy_ls.append({'model':'LSTM','AUC_SCORE':model_accuracy})\nmo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy_ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification on GRU(gated Recurrent Unit) Netwrok","metadata":{}},{"cell_type":"code","source":"%%time\n\nwith strategy.scope():\n    \n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                       300,\n                       weights = [embedding_metrics],\n                       input_length = max_len,\n                       trainable = False))\n    model.add(SpatialDropout1D(0.3))\n    model.add(GRU(300))\n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train_pad,ytrain, epochs = 5,batch_size = 64*strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru_pred = model.predict(x_valid_pad)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy = roc_auc_score(yvalid,gru_pred)\nmodel_accuracy_ls.append({'model':'GRU','AUC_SCORE':model_accuracy})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy_ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification with BiDirectional RNN's","metadata":{}},{"cell_type":"code","source":"%%time\n\nwith strategy.scope():\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                       300,\n                       weights = [embedding_metrics],\n                       input_length = max_len,\n                       trainable = False))\n    model.add(Bidirectional(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3)))\n    model.add(Dense(1,activation = 'sigmoid'))\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics = ['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train_pad, ytrain, epochs = 5, batch_size = 64* strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bidirectional_score = model.predict(x_valid_pad)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy = roc_auc_score(yvalid,bidirectional_score)\nmodel_accuracy_ls.append({'model':'Bidircetional_RNN','AUC_SCORE':model_accuracy})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualization of results obtained from various Deep learning Model\nresults_df = pd.DataFrame(model_accuracy_ls).sort_values(by = 'AUC_SCORE', ascending = False)\nresults_df.style.background_gradient(cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT Model","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n\n#save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n\n#reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt',lowercase = False)\nfast_tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = fast_encode(train.comment_text.astype(str), fast_tokenizer, maxlen=192)\nx_valid = fast_encode(validation.comment_text.astype(str), fast_tokenizer, maxlen=192)\nx_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=192)\n\ny_train = train.toxic.values\ny_valid = validation.toxic.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 512\nAUTO = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    funtion for training Bert Model\n    \"\"\"\n    \n    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:,0,:]\n    out = Dense(1, activation = 'sigmoid')(cls_token)\n    model = Model(inputs = input_word_ids, outputs = out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start model training \nwith strategy.scope():\n    transformer_layer = (\n    transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer,max_len = 192)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_steps = x_train.shape[0] # BATCH_SIZE\ntrain_history = model.fit(train_dataset,\n                         steps_per_epoch = n_steps,\n                         validation_data=valid_dataset,\n                         epochs=2\n                         )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_1 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=2*2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}